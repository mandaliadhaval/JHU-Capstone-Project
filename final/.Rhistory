library(tensorflow)
install.packages(tensorflow)
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
install_github("rstudio/tensorflow")
install.packages(devtools)
install.packages("devtools")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
install.packages("ggplot2")
install.packages("wordcloud")
knitr::opts_chunk$set(echo = TRUE)
## Prerequisites
if(!require(tm)) install.packages("tm", dep=T)
library(tm)
if(!require(SnowballC)) install.packages("SnowballC", dep=T)
library(SnowballC)
if(!require(Rgraphviz)) {
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
}
if(!require(qdap)) install.packages("qdap", dep=T)
library(qdap)
if(!require(RWeka)) install.packages("RWeka", dep=T)
library(RWeka)
library(Rgraphviz)
library(ggplot2)
library(wordcloud)
library(dplyr)
library(slam)
# custom functions for text prediction
source("textPrediction.R")
expandContractions <- function(doc) {
doc <- gsub("won't", "will not", doc) # a special case of "n't"
doc <- gsub("can't", "can not", doc) # another special case of "n't"
doc <- gsub("n't", " not", doc)
doc <- gsub("'ll", " will", doc)
doc <- gsub("'re", " are", doc)
doc <- gsub("'ve", " have", doc)
doc <- gsub("'m", " am", doc)
doc <- gsub("it's", "it is", doc) # a special case of 's
##doc <- gsub("'s", "", doc) # otherwise, possessive w/ no expansion
return(doc)
}
## custom transformation - specified texts to spaces
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
## custom transformation - UTF-8 to ASCII (remove special characters)
removeSpecial <- content_transformer(function(x)
iconv(x, "ASCII", "UTF-8", sub=""))
## given a set of texts, apply cleaning transformations
## and return a tm corpus containing the documents
##
createCleanCorpus <- function(texts, remove.punct=TRUE, remove.profanity=FALSE, profanity=NULL) {
texts <- expandContractions(texts)
filtered <- VCorpus(VectorSource(texts))
# remove digits
filtered <- tm_map(filtered, removeNumbers)
# substitute slashes, @'s and pipes to spaces
filtered <- tm_map(filtered, toSpace, "/|@|\\|")
# remove special characters
filtered <- tm_map(filtered, removeSpecial)
# convert to lower case
filtered <- tm_map(filtered, content_transformer(tolower))
# conditionally remove punctuation
if(remove.punct) {
filtered <- tm_map(filtered, removePunctuation,
preserve_intra_word_dashes=TRUE)
}
# remove profanity
if(remove.profanity) {
filtered <- tm_map(filtered, removeWords, profanity)
}
# strip excess whitespace
filtered <- tm_map(filtered, stripWhitespace)
}
### plotting - ngrams
## ngram plotting function
plotGram <- function(threshold, freq, wf, type) {
ggplot(subset(wf, freq >= wf$freq[threshold]),
aes(reorder(word, freq), freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
ggtitle(paste("Most Common ", type, "s", sep="")) +
xlab(type) + ylab("Frequency")
}
### prediction
## count the number of words in the character string provided
##
wordCount <- function(text) {
length(unlist(strsplit(text, " ")))
}
## return a string containing the last 'n' words of text
##   text - a string of characters containing words
##   n - the number of words to extract
##
## returns a string of characters containing the last 'n' words
##
lastWords <- function(text, n) {
paste(tail(unlist(strsplit(text, " ")), n), collapse=" ")
}
## return, ordered by frequency, all the n-grams starting with 'words'
##   words - a string of characters containing words to search for
##   nf - a dataframe of n-gram frequencies to search
##
## returns a vector containing up to count suggested next words
##
findBestMatches <- function(words, nf, count) {
# determine the size of the ngrams provided
nf.size <- length(unlist(strsplit(as.character(nf$word[1]), " ")))
# drop leading words longer than the ngrams
words.pre <- lastWords(words, nf.size - 1)
# matching ngrams that start with the provided words
f <- head(nf[grep(paste("^", words.pre, " ", sep=""), nf$word), ], count)
# strip away the search words from all the results
r <- gsub(paste("^", words.pre, " ", sep=""), "", as.character(f$word))
# filter incomplete word suggestions and filtering artifacts
r[!r %in% c("s", "<", ">", ":", "-", "o", "j", "c", "m")]
}
## given an input text, return the predicted next word
##   text - a character string containing words
##   nfl - n-gram frequency dataframes list
##
## returns a character string containing the predicted next word
##
predictNextWord <- function(text, nfl, count=1) {
text.wc <- wordCount(text)
prediction <- NULL
if(text.wc > 3) prediction <- findBestMatches(text, nfl$f5, count)
if(length(prediction)) return(prediction)
if(text.wc > 2) prediction <- findBestMatches(text, nfl$f4, count)
if(length(prediction)) return(prediction)
if(text.wc > 1) prediction <- findBestMatches(text, nfl$f3, count)
if(length(prediction)) return(prediction)
prediction <- findBestMatches(text, nfl$f2, count)
if(length(prediction)) return(prediction)
## text not found in any length n-grams?? randomly select from
## highest frequency words
as.character(sample(head(nfl$f1$word, nfl$r), count))
}
## clean the input text and perform prediction
cleanPredictNextWord <- function(text, nfl, count=1) {
text <- as.character(createCleanCorpus(text)[[1]], remove.punct=TRUE)
predictNextWord(text, nfl, count)
}
## given an input text, predict the current word
##   text - a character string containing a portion of a word
##   nfl - n-gram frequency dataframes list
##
## returns a character string containing the predicted current word
##
predictCurrentWord <- function(text, nfl, count=1) {
current <- as.character(createCleanCorpus(lastWords(text, 1))[[1]])
nf <- nfl$f1
# matching ngrams that start with the provided letters
f <- head(nf[grep(paste("^", current, sep=""), nf$word), ], count)
as.character(head(f$word, count))
}
### testing
## test the timing and accuracy of predictions on a set of test strings
##   corpus.test - a tm corpus of test strings
##   nfl - n-gram frequency dataframes list
## assumes that corpus.test has had createCleanCorpus() applied to it
## returns the accuracy as the fraction correctly predicted and the proc.time
##
testTimeAccuracy <- function(corpus.test, nfl, count=1) {
# extract a random substring of the provided text
#   text - a string of characters containing words
# returns both a substring and the actual next word, for prediction testing
randomSubstring <- function(text) {
# convert characters to a vector
wv <- unlist(strsplit(text, " "))
wv.start <- as.integer(runif(1, 1, length(wv) - 1))
wv.length <- as.integer(runif(1, 1, length(wv) - wv.start + 1))
wv.sub <- paste(wv[wv.start:(wv.start + wv.length - 1)], collapse=" ")
wv.next <- paste(wv[(wv.start + wv.length):(wv.start + wv.length)],
collapse=" ")
list("sub"=wv.sub, "nxt"=wv.next)
}
ptm <- proc.time()
success <- 0
invalid <- 0
for(i in 1:length(corpus.test)) {
testText <- corpus.test[[i]]$content
# exclude testing texts with only a single word (e.g. nothing to predict!)
if(wordCount(testText) > 1) {
ts <- randomSubstring(testText)
if(ts$nxt %in% cleanPredictNextWord(ts$sub, nfl, count))
success = success + 1
}
else {
invalid <- invalid + 1 # count of invalid tests
}
}
accuracy <- success / (length(corpus.test) - invalid)
time <- proc.time() - ptm
list("accuracy"=accuracy, "time"=time)
}
## Prerequisites
if(!require(tm)) install.packages("tm", dep=T)
library(tm)
if(!require(SnowballC)) install.packages("SnowballC", dep=T)
library(SnowballC)
if(!require(Rgraphviz)) {
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
}
if(!require(qdap)) install.packages("qdap", dep=T)
library(qdap)
if(!require(RWeka)) install.packages("RWeka", dep=T)
library(RWeka)
library(Rgraphviz)
library(ggplot2)
library(wordcloud)
library(dplyr)
library(slam)
# custom functions for text prediction
source("textPrediction.R")
setwd("D:/Documents/Training/John Hopkins Data Science/Course 10 - Capstone Project/Project")
# view the English sample text source documents
cpath <- file.path(".", "final", "en_US")
csize <- length(dir(cpath))
dir(cpath)
wc <- function(ctree, corpus) {
unlist(strsplit(sub("^ +", "",
system(paste("wc ", ctree, corpus, sep=""),
intern=TRUE)), split=" +"))
}
ctree <- "final/en_US/en_US."
wc.blogs <- wc(ctree, "blogs.txt")
wc <- function(ctree, corpus) {
unlist(strsplit(sub("^ +", "",
system(paste("wc ", ctree, corpus, sep=""),
intern=TRUE)), split=" +"))
}
ctree <- "/final/en_US/en_US."
wc.blogs <- wc(ctree, "blogs.txt")
wc <- function(ctree, corpus) {
unlist(strsplit(sub("^ +", "",
system(paste("wc ", ctree, corpus, sep=""),
intern=TRUE)), split=" +"))
}
ctree <- "final/en_US/en_US."
wc.blogs <- wc(ctree, "blogs.txt")
system(paste("wc ", ctree, corpus, sep="")
)
system(paste("wc ", ctree, "blogs.txt", sep=""))
temp<-system(paste("wc ", ctree, "blogs.txt", sep=""))
temp
View(temp)
temp<-unlist(strsplit(sub("^ +", "",system(paste("wc ", ctree, "blogs.txt", sep=""),intern=TRUE)), split=" +"))
temp<-system(paste("wc ", ctree, "blogs.txt", sep=""),intern=TRUE)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", skipNul=TRUE)
texts.full <- c(blogs, news, twitter)
name <- c("blog", "twitter", "news", "all")
bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
corpus.info <- data.frame(name, bytes, lines)
rm(blogs, twitter, news) # remove separate stores for each type
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
profanity <- as.character(read.csv("/final/en_US/Profanity.txt", header=FALSE)$V1)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
source("textPrediction.R")
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
source("textPrediction.R")
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
rm(createCleanCorpus)
source("textPrediction.R")
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", skipNul=TRUE)
texts.full <- c(blogs, news, twitter)
name <- c("blog", "twitter", "news", "all")
bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
corpus.info <- data.frame(name, bytes, lines)
rm(blogs, twitter, news) # remove separate stores for each type
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", skipNul=TRUE)
texts.full <- c(blogs, news, twitter)
name <- c("blog", "twitter", "news", "all")
bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
corpus.info <- data.frame(name, bytes, lines)
rm(blogs, twitter, news) # remove separate stores for each type
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
news <- readLines("final/en_US/en_US.news.txt", skipNul=TRUE)
texts.full <- c(blogs, news, twitter)
name <- c("blog", "twitter", "news", "all")
bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
corpus.info <- data.frame(name, bytes, lines)
rm(blogs, twitter, news) # remove separate stores for each type
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
texts.training <- sample(texts.full, 10000, replace=FALSE)
texts.testing <- sample(texts.full, 1000, replace=FALSE)
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
source("textPrediction.R")
profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
# for unigrams, remove punctuation
filtered.sub.np <- createCleanCorpus(texts.training,
remove.punct=TRUE, remove.profanity=TRUE, profanity)
# for generating predictive corpus, leave punctuation. tm/dtm uses.
filtered.sub <- createCleanCorpus(texts.training,
remove.punct=FALSE, remove.profanity=TRUE, profanity)
# for generating test text, remove punctuation
filtered.test <- createCleanCorpus(texts.testing, remove.punct=TRUE)
# sentence delimiters; prevent clustering across sentence boundaries
delimiters <- " \\t\\r\\n.!?,;\"()"
# n-gram tokenizers
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
TrigramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=3, max=3))
QuadgramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=4, max=4))
PentagramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=5, max=5))
gthreshold <- 15 # threshold for number of n-grams to display graphically
options(mc.cores=1) # limit cores to prevent rweka processing problems
ft.1 <- 10
dtm.1 <- DocumentTermMatrix(filtered.sub.np, control=list(minDocFreq=ft.1))
freq.1 <- sort(colSums(as.matrix(dtm.1)), decreasing=TRUE)
nf.1 <- data.frame(word=names(freq.1), freq=freq.1)
plotGram(gthreshold, freq.1, nf.1, "Word")
ft.2 <- 3
dtm.2 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=BigramTokenizer, bounds=list(global=c(ft.2, Inf))))
freq.2 <- sort(col_sums(dtm.2, na.rm=T), decreasing=TRUE)
nf.2 <- data.frame(word=names(freq.2), freq=freq.2)
plotGram(gthreshold, freq.2, nf.2, "2-gram")
ft.3 <- 3
dtm.3 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=TrigramTokenizer, bounds=list(global=c(ft.3, Inf))))
freq.3 <- sort(col_sums(dtm.3, na.rm=T), decreasing=TRUE)
nf.3 <- data.frame(word=names(freq.3), freq=freq.3)
plotGram(gthreshold, freq.3, nf.3, "3-gram")
ft.4 <- 2
dtm.4 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=QuadgramTokenizer, bounds=list(global=c(ft.4, Inf))))
freq.4 <- sort(col_sums(dtm.4, na.rm=T), decreasing=TRUE)
nf.4 <- data.frame(word=names(freq.4), freq=freq.4)
plotGram(gthreshold, freq.4, nf.4, "4-gram")
ft.5 <- 2
dtm.5 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=PentagramTokenizer, bounds=list(global=c(ft.5, Inf))))
freq.5 <- sort(col_sums(dtm.5, na.rm=T), decreasing=TRUE)
nf.5 <- data.frame(word=names(freq.5), freq=freq.5)
plotGram(gthreshold, freq.5, nf.5, "5-gram")
r <- 10 # frequency span for last-resort randomization
nf <- list("f1"=nf.1, "f2"=nf.2, "f3"=nf.3, "f4"=nf.4, "f5"=nf.5, "r"=r)
save(nf, file="data/nFreq.Rda") # save the ngram frequencies to disk
r <- 10 # frequency span for last-resort randomization
nf <- list("f1"=nf.1, "f2"=nf.2, "f3"=nf.3, "f4"=nf.4, "f5"=nf.5, "r"=r)
save(nf, file="final/nFreq.Rda") # save the ngram frequencies to disk
load("final/nFreq-200000-10-3-3-2-2.Rda")
load("final/nFreq.Rda")
# return the number of entries with frequency exceeding count
countAboveFrequency <- function(nf, count) {
dim(nf[nf$freq > count, ])[1]
}
set.seed(482)
wordcloud(names(
c(
freq.1[freq.1 > freq.1[50]],
freq.2[freq.2 > freq.2[50]],
freq.3[freq.3 > freq.3[50]],
freq.4[freq.4 > freq.4[50]],
freq.5[freq.5 > freq.5[50]])),
c(
freq.1[freq.1 > freq.1[50]],
freq.2[freq.2 > freq.2[50]],
freq.3[freq.3 > freq.3[50]],
freq.4[freq.4 > freq.4[50]],
freq.5[freq.5 > freq.5[50]]),
max.words=200, colors=brewer.pal(8, "Dark2"), rot.per=0.35, scale=c(6, 0.5))
set.seed(2572)
test.result.1 <- testTimeAccuracy(filtered.test, nf, 1)
set.seed(2572)
test.result.5 <- testTimeAccuracy(filtered.test, nf, 5)
predictNextWord("rock and a hard", nf)
* was having a hard -> **`r predictNextWord("rock and a hard", nf)`**
predictNextWord("rock and a hard", nf)
predictNextWord("say thanks for the", nf)
predictNextWord("a few years", nf)
predictNextWord("the first time", nf)
predictNextWord("i am so", nf)
predictNextWord("be a", nf)
predictNextWord("can not", nf)
predictNextWord("no matter", nf)
predictNextWord("a", nf)
predictNextWord("will", nf)
predictNextWord("could", nf)
predictNextWord("xxxxxxxx", nf)
length(filtered.test)
test.result.1
test.result.5
round(test.result.5$accuracy * 100, digits=2)
sprintf("%.1f", test.result.1$time[1])`ms
sprintf("%.1f", test.result.1$time[1])
setwd("D:/Documents/Training/John Hopkins Data Science/Course 10 - Capstone Project/Project/final")
shiny::runApp('Word_Predictor_App')
ls
runApp('Word_Predictor_App')
