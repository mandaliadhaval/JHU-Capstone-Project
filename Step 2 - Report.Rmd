---
title: "Report"
author: "Dhaval Mandalia"
date: "March 5, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text Prediction (Exploratory Analysis and Prediction)

## Summary

This document summarizes work done to construct, test and optimize a model for text prediction.

A body of sample texts consisting of ~4M documents including tweets, news articles and blog posts are loaded and exploratory analysis performed. Sets of n-grams are extracted from the body of text, predictive algorithms built, and various approaches for improving predictive accuracy refined. 

A cursory analysis of the dataset was presented in the [https://github.com/pchuck/coursera-ds-capstone/blob/master/milestone.md](Milestone 1) report.

This is the final capstone project for the Johns Hopkins data science specialization certification series. The corpus for the analysis is available at [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).


```{r prereqs, message=FALSE, warning=FALSE, echo=FALSE}
## Prerequisites
  if(!require(tm)) install.packages("tm", dep=T)
  library(tm)
  if(!require(SnowballC)) install.packages("SnowballC", dep=T)
  library(SnowballC)
  if(!require(Rgraphviz)) {
    source("http://bioconductor.org/biocLite.R")
    biocLite("Rgraphviz")
  }
  if(!require(qdap)) install.packages("qdap", dep=T)
  library(qdap)
  if(!require(RWeka)) install.packages("RWeka", dep=T)
  library(RWeka)
  
  library(Rgraphviz)
  library(ggplot2)
  library(wordcloud)
  library(dplyr)
  library(slam)
  # custom functions for text prediction
  source("textPrediction.R")
```

## Load and Examine the Sample Texts

### Documents

```{r corpus.find, echo=FALSE}
  # view the English sample text source documents
  cpath <- file.path(".", "final", "en_US")
  csize <- length(dir(cpath))
  dir(cpath)
```

```{r corpus.wc, echo=FALSE}
  wc <- function(ctree, corpus) {
    unlist(strsplit(sub("^ +", "",
      system(paste("wc ", ctree, corpus, sep=""),
      intern=TRUE)), split=" +"))
  }
  ctree <- "final/en_US/en_US."
  wc.blogs <- wc(ctree, "blogs.txt")
  wc.news <- wc(ctree, "news.txt")
  wc.twitter <- wc(ctree, "twitter.txt")
  wc.total.m <- round((as.numeric(wc.blogs[2]) +
                       as.numeric(wc.news[2]) +
                       as.numeric(wc.twitter[2]))/1000/1000, digits=0)
```
The English-language content is used for the analysis. `r csize` document sets are ingested.

* __blogs__ contains `r wc.blogs[1]` lines, `r wc.blogs[2]` words, and `r wc.blogs[3]` characters.
* __twitter__ contains `r wc.twitter[1]` lines, `r wc.twitter[2]` words, and `r wc.twitter[3]` characters.
* __news__ contains `r wc.news[1]` lines, `r wc.news[2]` words, and `r wc.news[3]` characters.

```{r corpus.load, cache=TRUE, echo=FALSE}
  blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul=TRUE)
  twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul=TRUE)
  news <- readLines("final/en_US/en_US.news.txt", skipNul=TRUE)
  texts.full <- c(blogs, news, twitter)
  name <- c("blog", "twitter", "news", "all")
  bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
  lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
  corpus.info <- data.frame(name, bytes, lines)
  rm(blogs, twitter, news) # remove separate stores for each type
```

```{r corpus.sub.load, cache=TRUE, echo=FALSE}
  texts.training <- sample(texts.full, 10000, replace=FALSE)
  texts.testing <- sample(texts.full, 1000, replace=FALSE)
```

```{r corpus.clean, echo=FALSE, cache=TRUE}
  profanity <- as.character(read.csv("final/en_US/Profanity.txt", header=FALSE)$V1)
  # for unigrams, remove punctuation
  filtered.sub.np <- createCleanCorpus(texts.training,
    remove.punct=TRUE, remove.profanity=TRUE, profanity)
  # for generating predictive corpus, leave punctuation. tm/dtm uses.
  filtered.sub <- createCleanCorpus(texts.training,
    remove.punct=FALSE, remove.profanity=TRUE, profanity)
  # for generating test text, remove punctuation
  filtered.test <- createCleanCorpus(texts.testing, remove.punct=TRUE)
```  


## Exploratory Analysis

Document-term matrices are created and n-grams ranging in sequence length
from 1 to 5 words are created for the purpose of analyzing word frequencies
and various characteristics of the dataset.


### N-grams

n-grams are extracted to characterize the frequency of multi-word
clusters.

```{r explore.ngrams, cache=TRUE}
  # sentence delimiters; prevent clustering across sentence boundaries
  delimiters <- " \\t\\r\\n.!?,;\"()"
  # n-gram tokenizers
  BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
  TrigramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=3, max=3))
  QuadgramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=4, max=4))
  PentagramTokenizer <- function(x, n) NGramTokenizer(x, Weka_control(min=5, max=5))
 
  gthreshold <- 15 # threshold for number of n-grams to display graphically
  options(mc.cores=1) # limit cores to prevent rweka processing problems
  ft.1 <- 10
  dtm.1 <- DocumentTermMatrix(filtered.sub.np, control=list(minDocFreq=ft.1))  
  freq.1 <- sort(colSums(as.matrix(dtm.1)), decreasing=TRUE)
  nf.1 <- data.frame(word=names(freq.1), freq=freq.1)
  plotGram(gthreshold, freq.1, nf.1, "Word")
  
  ft.2 <- 3
  dtm.2 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=BigramTokenizer, bounds=list(global=c(ft.2, Inf))))
  freq.2 <- sort(col_sums(dtm.2, na.rm=T), decreasing=TRUE)
  nf.2 <- data.frame(word=names(freq.2), freq=freq.2)
  plotGram(gthreshold, freq.2, nf.2, "2-gram")
  
  ft.3 <- 3
  dtm.3 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=TrigramTokenizer, bounds=list(global=c(ft.3, Inf))))
  freq.3 <- sort(col_sums(dtm.3, na.rm=T), decreasing=TRUE)
  nf.3 <- data.frame(word=names(freq.3), freq=freq.3)
  plotGram(gthreshold, freq.3, nf.3, "3-gram")
  
  ft.4 <- 2 
  dtm.4 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=QuadgramTokenizer, bounds=list(global=c(ft.4, Inf))))
  freq.4 <- sort(col_sums(dtm.4, na.rm=T), decreasing=TRUE)
  nf.4 <- data.frame(word=names(freq.4), freq=freq.4)
  plotGram(gthreshold, freq.4, nf.4, "4-gram")
  
  ft.5 <- 2
  dtm.5 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=PentagramTokenizer, bounds=list(global=c(ft.5, Inf))))
  freq.5 <- sort(col_sums(dtm.5, na.rm=T), decreasing=TRUE)
  nf.5 <- data.frame(word=names(freq.5), freq=freq.5)
  plotGram(gthreshold, freq.5, nf.5, "5-gram")
```

### Final/Optimized Dataset

```{r save.ngrams, eval=FALSE}
  r <- 10 # frequency span for last-resort randomization
  nf <- list("f1"=nf.1, "f2"=nf.2, "f3"=nf.3, "f4"=nf.4, "f5"=nf.5, "r"=r)
  save(nf, file="final/nFreq.Rda") # save the ngram frequencies to disk
```

Generating the most common n-grams from even a subset (200K documents) of the
full corpus can take several hours. Here, it is saved in a previous
session and then loaded from disk:

```{r load.ngrams, eval=TRUE}
  load("final/nFreq.Rda")
```

### N-Gram Distribution

```{r optimization.count}
  # return the number of entries with frequency exceeding count
  countAboveFrequency <- function(nf, count) {
    dim(nf[nf$freq > count, ])[1]
  }
```

#### Total Count (Unique)
  * 5-grams: **`r countAboveFrequency(nf.5, 0)`** (w/ frequency > 2)
  * 4-grams: **`r countAboveFrequency(nf.4, 0)`** (w/ frequency > 2)
  * 3-grams: **`r countAboveFrequency(nf.3, 0)`** (w/ frequency > 3)
  * 2-grams: **`r countAboveFrequency(nf.2, 0)`** (w/ frequency > 3)
  * words: **`r countAboveFrequency(nf.1, 0)`** (w/ frequency > 10)


#### Word Cloud

A word cloud can be used to show the most frequently occurring words and {2, 3, 4, 5}-grams.

```{r explore.terms.wc, echo=FALSE}
  set.seed(482)
  wordcloud(names(
        c(
        freq.1[freq.1 > freq.1[50]],
        freq.2[freq.2 > freq.2[50]],
        freq.3[freq.3 > freq.3[50]],
        freq.4[freq.4 > freq.4[50]],
        freq.5[freq.5 > freq.5[50]])),
        c(
        freq.1[freq.1 > freq.1[50]],
        freq.2[freq.2 > freq.2[50]],
        freq.3[freq.3 > freq.3[50]],
        freq.4[freq.4 > freq.4[50]],
        freq.5[freq.5 > freq.5[50]]),
        max.words=200, colors=brewer.pal(8, "Dark2"), rot.per=0.35, scale=c(6, 0.5))
```

## Prediction

### Unit Tests

Here are some simple tests to verify sane predictions for n-gram input phrases. The last word in each phrase is provided by the prediction function.

#### 5-grams
* was having a hard -> **`r predictNextWord("rock and a hard", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("rock and a hard", nf)
```

#### 4-grams
* thanks for the -> **`r predictNextWord("say thanks for the", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("say thanks for the", nf)
```

* a few years -> **`r predictNextWord("a few years", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("a few years", nf)
```
* the first time -> **`r predictNextWord("the first time", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("the first time", nf)
```
* i am so -> **`r predictNextWord("i am so", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("i am so", nf)
```

#### 3-gram matches
* be a -> **`r predictNextWord("be a", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("be a", nf)
```
* can not -> **`r predictNextWord("can not", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("can not", nf)
```
* no matter -> **`r predictNextWord("no matter", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("no matter", nf)
```

#### 2-gram matches
* a -> **`r predictNextWord("a", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("a", nf)
```
* will -> **`r predictNextWord("will", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("will", nf)
```
* could -> **`r predictNextWord("could", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("could", nf)
```

#### non-match (***resorts to a pick from common 1-grams***)
* xxxxxxxx -> **`r predictNextWord("xxxxxxxx", nf)`**
```{r explore.terms.wc, echo=FALSE}
predictNextWord("xxxxxxxx", nf)
```


## Algorithm and Optimizations

### Algorithm

The following algorithm is applied for next word prediction

1. Capture input text, including all preceding words in the phrase
2. Iteratively traverse n-grams (longest to shortest) for matching strings
3. On match(s), use the longest, highest frequency occurring, n-gram
4. The last word in the matching n-gram is the predicted next word
5. If no matches found in n-grams, randomly select a common word from 1-grams

### Preprocessing

The following preprocessing steps were applied to create a set of
n-grams that  could be traversed in the search for a match with
the input phrase.

1. Convert texts to a DTM (document-term matrix)
    * full corpus (4.2M lines) stored as DTM is ~16GB
    * too large to manipulate in memory...
2. Reduce corpus size by sampling a 1/4 subset 
    * 1M documents can be stored and manipulated as a ~4GB DTM
3. Perform basic text filtering
    * remove digits
    * convert special characters (e.g. /'s, @'s and |'s) to whitespace
    * remove other special characters
    * convert to lower case
    * remove punctuation (for 1-grams), keep (for other n-grams)
    * remove excess whitespace
4. From the transformed/filtered DTM, generate n-grams
    * use {1, 2, 3, 4}-grams as a starting point

### Optimizations 

The algorithm depends on the existence of a set of n-grams which
is large enough to contain a good sampling of word combinations but
small enough to be searched in a fraction of a second. The following
optimizations were tested in the pursuit of finding a reasonable balance
between accuracy and prediction speed. For each combination, accuracy,
execution time and dataset size were recorded.

5. Initial attempt to generate {1, 2, 3, 4}-grams
    * Tokenization of 4GB DTM still too memory/compute intensive
    * Resulting sparse matrix X*Y dimensions too large
    * Work around RWeka errors with "options(mc.cores=1)"
6. Subsample DTM down further to 10K documents and generate {1, 2, 3, 4}-grams
    * **Result: 10.1% accuracy, 8ms response time, 9.4MB dataset**
7. Add 5-grams in attempt to improve accuracy
    * **Result: 8.4% accuracy, 285ms response time, 14.1MB dataset**
    * Worse accuracy? Maybe the corpus is too small..
8. Increase size of sampled corpus (from 10K to 50K docs) and drop n-grams which don't occur at least once to improve accuracy and reduce DTM size
    * **Result: 10.8% accuracy, 34ms response time, 1.0MB dataset**
    * Better accuracy, pruning the n-grams seems promising..
9. Further prune low-frequency n-grams (<10/6/4/3/2 occurrences)
    * **Result: 11.8% accuracy, 27ms response time, 0.7MB dataset**
10. Increase size of sampled corpus (from 50K to 100K docs)
    * **Result: 12.3% accuracy, 37ms response time, 1.2MB dataset**
11. Further increase size of sampled corpus (from 100K to 200K docs)
    * **Result: 15.3% accuracy, 395ms response time, 11.7MB dataset**
12. Apply profanity filter
    * **Result: 15.3% accuracy, 400ms response time, 11.6MB dataset**
13. Prune 2 and 3-grams occurring less than 3-times to improve performance
    * **Result: 15.2% accuracy, 257ms response time, 9.1MB dataset**

### Final, Optimized N-grams

* 4M corpus reduced to 1M documents via random sampling
* 1M documents cleaned/transformed and reduced to 200K subset
* Document-term matrix generated and n-grams up to depth 5 extracted
* n-grams organized by frequency/# of occurrences in corpus
* Least common n-grams pruned/dropped, resulting in final dataset
    * Optimized N-grams: **9.1MB** compressed, **104MB** in-memory
    * 18,936 words occurring more than 10x
    * 199,966 2-grams w/ frequency > 3x
    * 150,489 3-grams w/ frequency > 3x
    * 139,984 4-grams w/ frequency > 2x
    * 43,024 5-grams w/ frequency > 2x

## Conclusions

Through a series of iterations of exploratory analysis, refinements and
testing, predictive accuracy was improved from 8% to 15% while maintaining
a response time suitable for interactive use (<300ms) and producing
a compressed and optimized dataset under 10MB in size.

### Accuracy

As a final accuracy test, **`r length(filtered.test)`** random phrases of
varying length were extracted from
the testing text set and the last word of each sequence excluded.
The word prediction model was then invoked on each test phrase and the
predicted word compared to the actual (excluded) word from the phrase.

```{r predict.accuracy, cache=TRUE, echo=FALSE}
  set.seed(2572)
  test.result.1 <- testTimeAccuracy(filtered.test, nf, 1)
```
The measured accuracy of the model (using only the 1st, top-ranked, response) is **`r round(test.result.1$accuracy * 100, digits=2)`%**.
  
```{r predict.accuracy.2, cache=TRUE, echo=FALSE}
  set.seed(2572)
  test.result.5 <- testTimeAccuracy(filtered.test, nf, 5)
```
  The measured accuracy of the model (using top-5 ranked responses) is **`r round(test.result.5$accuracy * 100, digits=2)`%**.

### Performance

The average speed of the algorithm is **`r sprintf("%.1f", test.result.1$time[1])`ms** per word prediction.


