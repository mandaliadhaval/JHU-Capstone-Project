repo_summary <- repo_summary %>% mutate(pct_n_char = round(n_char/sum(n_char), 2))
repo_summary <- repo_summary %>% mutate(pct_lines = round(f_lines/sum(f_lines), 2))
repo_summary <- repo_summary %>% mutate(pct_words = round(n_words/sum(n_words), 2))
kable(repo_summary)
saveRDS(repo_summary, "./clean_repos/repo_summary.rds")
saveRDS(repo_summary, "./clean_repos/repo_summary.rds")
repo_summary <- data.frame(f_names = c("blogs", "news", "twitter"),
f_size  = c(blogs_size, news_size, twitter_size),
f_lines = c(blogs_lines, news_lines, twitter_lines),
n_char =  c(blogs_nchar_sum, news_nchar_sum, twitter_nchar_sum),
n_words = c(blogs_words, news_words, twitter_words))
repo_summary <- repo_summary %>% mutate(pct_n_char = round(n_char/sum(n_char), 2))
repo_summary <- repo_summary %>% mutate(pct_lines = round(f_lines/sum(f_lines), 2))
repo_summary <- repo_summary %>% mutate(pct_words = round(n_words/sum(n_words), 2))
kable(repo_summary)
saveRDS(repo_summary, "./clean_repos/repo_summary.rds")
blogs   <- data_frame(text = blogs)
news    <- data_frame(text = news)
twitter <- data_frame(text = twitter)
set.seed(1001)
sample_pct <- 0.1
blogs_sample <- blogs %>%
sample_n(., nrow(blogs)*sample_pct)
news_sample <- news %>%
sample_n(., nrow(news)*sample_pct)
twitter_sample <- twitter %>%
sample_n(., nrow(twitter)*sample_pct)
#' Create tidy repository
repo_sample <- bind_rows(mutate(blogs_sample, source = "blogs"),
mutate(news_sample,  source = "news"),
mutate(twitter_sample, source = "twitter"))
repo_sample$source <- as.factor(repo_sample$source)
data("stop_words")
swear_words <- read_delim("./Resources/config/en_US/Profanity.txt", delim = "\n", col_names = FALSE)
swear_words <- unnest_tokens(swear_words, word, X1)
replace_reg <- "[^[:alpha:][:space:]]*"
replace_url <- "http[^[:space:]]*"
replace_aaa <- "\\b(?=\\w*(\\w)\\1)\\w+\\b"
clean_sample <-  repo_sample %>%
mutate(text = str_replace_all(text, replace_reg, "")) %>%
mutate(text = str_replace_all(text, replace_url, "")) %>%
mutate(text = str_replace_all(text, replace_aaa, "")) %>%
mutate(text = iconv(text, "ASCII//TRANSLIT"))
rm(blogs, blogs_nchar, news, news_nchar, twitter, twitter_nchar, replace_reg, replace_url, replace_aaa)
#' Create tidy dataframe for repo sample
tidy_repo <- clean_sample %>%
unnest_tokens(word, text) %>%
anti_join(swear_words) %>%
anti_join(stop_words)
(repo_count <- tidy_repo %>%
summarise(keys = n_distinct(word)))
#' Number of words to attain 50% and 90% coverage of all words in repo
cover_50 <- tidy_repo %>%
count(word) %>%
mutate(proportion = n / sum(n)) %>%
arrange(desc(proportion)) %>%
mutate(coverage = cumsum(proportion)) %>%
filter(coverage <= 0.5)
nrow(cover_50)
cover_90 <- tidy_repo %>%
count(word) %>%
mutate(proportion = n / sum(n)) %>%
arrange(desc(proportion)) %>%
mutate(coverage = cumsum(proportion)) %>%
filter(coverage <= 0.9)
nrow(cover_90)
cover_90 %>%
top_n(20, proportion) %>%
mutate(word = reorder(word, proportion)) %>%
ggplot(aes(word, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
#' Word distribution by source
freq <- tidy_repo %>%
count(source, word) %>%
group_by(source) %>%
mutate(proportion = n / sum(n)) %>%
spread(source, proportion) %>%
gather(source, proportion, `blogs`:`twitter`) %>%
arrange(desc(proportion), desc(n))
freq %>%
filter(proportion > 0.002) %>%
mutate(word = reorder(word, proportion)) %>%
ggplot(aes(word, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip() +
facet_grid(~source, scales = "free")
#' Word cloud
cover_90 %>%
with(wordcloud(word, n, max.words = 100,
colors = brewer.pal(6, 'Dark2'), random.order = FALSE))
saveRDS(tidy_repo, "./clean_repos/tidy_repo.rds")
saveRDS(cover_90, "./clean_repos/cover_90.rds")
rm(tidy_repo, cover_50, cover_90)
bigram_repo <- clean_sample  %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
#' Number of bigrams to attain 90% coverage of all bigrams in repo
bigram_cover_90 <- bigram_repo %>%
count(bigram) %>%
mutate(proportion = n / sum(n)) %>%
arrange(desc(proportion)) %>%
mutate(coverage = cumsum(proportion)) %>%
filter(coverage <= 0.9)
nrow(bigram_cover_90)
bigram_cover_90 %>%
top_n(20, proportion) %>%
mutate(bigram = reorder(bigram, proportion)) %>%
ggplot(aes(bigram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
saveRDS(bigram_cover_90, "./clean_repos/bigram_cover_90.rds")
trigram_repo <- clean_sample  %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3)
#' Number of trigrams to attain 90% coverage of all trigrams in repo
trigram_cover_90 <- trigram_repo %>%
count(trigram) %>%
mutate(proportion = n / sum(n)) %>%
arrange(desc(proportion)) %>%
mutate(coverage = cumsum(proportion)) %>%
filter(coverage <= 0.9)
nrow(trigram_cover_90)
#' trigram distribution
trigram_cover_90 %>%
top_n(20, proportion) %>%
mutate(trigram = reorder(trigram, proportion)) %>%
ggplot(aes(trigram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
saveRDS(trigram_cover_90, "./clean_repos/trigram_cover_90.rds")
quadgram_repo <- clean_sample  %>%
unnest_tokens(quadgram, text, token = "ngrams", n = 4)
#' Number of quadgrams to attain 90% coverage of all quadgrams in repo
quadgram_cover_90 <- quadgram_repo %>%
count(quadgram) %>%
mutate(proportion = n / sum(n)) %>%
arrange(desc(proportion)) %>%
mutate(coverage = cumsum(proportion)) %>%
filter(coverage <= 0.9)
nrow(quadgram_cover_90)
quadgram_cover_90 %>%
top_n(20, proportion) %>%
mutate(quadgram = reorder(quadgram, proportion)) %>%
ggplot(aes(quadgram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
quadgrams_separated <- quadgram_cover_90 %>%
separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ")
quadgrams_separated
saveRDS(quadgram_cover_90, "./clean_repos/quadgram_cover_90.rds")
end <- Sys.time()
(run_time <- end - start_time)
sessionInfo()
repo_summary <- readRDS("./clean_repos/repo_summary.rds")
tidy_repo <- readRDS("./clean_repos/tidy_repo.rds")
cover_90  <- readRDS("./clean_repos/cover_90.rds")
bigram_cover_90   <- readRDS("./clean_repos/bigram_cover_90.rds")
trigram_cover_90  <- readRDS("./clean_repos/trigram_cover_90.rds")
quadgram_cover_90 <- readRDS("./clean_repos/quadgram_cover_90.rds")
#' Word cloud
cover_90 %>%
with(wordcloud(word, n, max.words = 100,
colors = brewer.pal(6, 'Dark2'), random.order = FALSE))
knitr::kable(repo_summary)
#' Word distribution by source
freq <- tidy_repo %>%
count(source, word) %>%
group_by(source) %>%
mutate(proportion = n / sum(n)) %>%
spread(source, proportion) %>%
gather(source, proportion, `blogs`:`twitter`) %>%
arrange(desc(proportion), desc(n))
freq %>%
filter(proportion > 0.002) %>%
mutate(word = reorder(word, proportion)) %>%
ggplot(aes(word, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip() +
facet_grid(~source, scales = "free")
#' Word distribution
cover_90 %>%
top_n(20, proportion) %>%
mutate(word = reorder(word, proportion)) %>%
ggplot(aes(word, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
#' Bigram distribution
bigram_cover_90 %>%
top_n(20, proportion) %>%
mutate(bigram = reorder(bigram, proportion)) %>%
ggplot(aes(bigram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
#' trigram distribution
trigram_cover_90 %>%
top_n(20, proportion) %>%
mutate(trigram = reorder(trigram, proportion)) %>%
ggplot(aes(trigram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
#' quadgram distribution
quadgram_cover_90 %>%
top_n(20, proportion) %>%
mutate(quadgram = reorder(quadgram, proportion)) %>%
ggplot(aes(quadgram, proportion)) +
geom_col() +
xlab(NULL) +
coord_flip()
quadgrams_separated <- quadgram_cover_90 %>%
separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ")
knitr::kable(head(quadgrams_separated))
rm(list=ls())
gc()
knitr::opts_chunk$set(echo = TRUE,eval=FALSE,fig.align='center', message = FALSE, warning=FALSE)
library(data.table)
library(tidyverse)
library(stringr)
library(qdap)
install.packages("qdap","tm","splines")
install.packages("qdap")
install.packages("tm")
install.packages(splines")
""
install.packages("splines")
library(data.table)
library(tidyverse)
library(stringr)
library(qdap)
install.packages("qdap")
library(data.table)
library(tidyverse)
library(stringr)
library(qdap)
library(tm)
library(tidytext)
library(wordcloud)
library(splines)
news <- read_lines("/Resources/corpus/en_US/en_US.news.txt") %>% tbl_df
setwd("D:/Documents/Training/John Hopkins Data Science/Course 10 - Capstone Project")
news <- read_lines("Resources/corpus/en_US/en_US.news.txt") %>% tbl_df
blogs <- read_lines("Resources/corpus/en_US/en_US.blogs.txt") %>% tbl_df
tweets <- read_lines("Resources/corpus/en_US/en_US.twitter.txt") %>% tbl_df
# 1o Helper Functions
get_med_bytes_per_row = function(text_df_source) {
return(apply(text_df_source, 1, object.size) %>% median)
}
# 2o Helper Functions
sample_text_data = function(sample_size_list, sources = list(news, blogs, tweets), seed = 123) {
all_sample = data.frame()
set.seed(seed)
for (i in 1:length(sources)) {
new_sample = sample_n(sources[[i]], sample_size_list[[i]])
all_sample = bind_rows(all_sample, new_sample)
}
return(all_sample)
}
profvis::profvis({vector_source = news$value[1:10000]
clean_output = vector_source[!str_detect(vector_source, "[^\x20-\x7E]")]
clean_output = clean_output %>% str_split('[?!.|;]+ ')
clean_output = clean_output %>% unlist
clean_output = clean_output %>% str_replace_all('[^\\sA-Za-z\'\\-]', '')
clean_output = clean_output %>% str_replace_all('(?<!\\b[A-Za-z]{0,9})([-])(?![A-Za-z]+\\b)', "")
#clean_output = clean_output %>% str_num2words
clean_output = clean_output[clean_output != ""]
})
profvis::profvis({
character_vector = clean_output
new_vec = c()
for (input_string in character_vector) {
## Given an input string, replaces all digits with their numeric equivalents using numbers2words()
numbers = str_extract_all(input_string, '[0-9]+')[[1]]
conv_list = list()
for(num in numbers) {
conv_list[[num]] = numbers2words(as.numeric(num))
}
output = input_string
for(num in names(conv_list)) {
output = str_replace_all(output, num, conv_list[[num]])
}
new_vec = c(new_vec, output)
}
})
# 1o Helper Functions
clean_vector_source = function(vector_source) {
clean_output = vector_source %>%
str_num2words %>%
str_replace_all('[^\\sA-Za-z\'\\-]', '') %>%
str_replace_all('(?<!\\b[A-Za-z]{0,9})([-])(?![A-Za-z]+\\b)', "")
clean_output = clean_output[!str_detect(clean_output, "[^\x20-\x7E]")]
return(clean_output)
}
get_all_grams = function(tbl_source) {
unigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 1, to_lower=FALSE)
unigrams$n_gram = 'one'
bigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 2, to_lower=FALSE)
bigrams$n_gram = 'two'
trigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 3, to_lower=FALSE)
trigrams$n_gram = 'three'
return(bind_rows(unigrams,bigrams,trigrams))
}
# 2o Helper Functions
get_gram_counts = function(vector_source) {
all_tokens = data.frame(clean=clean_vector_source(vector_source), stringsAsFactors = FALSE) %>%
tbl_df %>%
get_all_grams %>%
group_by(n_gram) %>%
count(token, sort=TRUE)
return(all_tokens)
}
# Main
build_token_count_df = function(seed=1234, n_samples = 50, sample_size = 1*10^7) {
set.seed(seed)
sample_size_list = lapply(list(news, blogs, tweets), function(x) sample_size / get_med_bytes_per_row(x))
final_output = data.frame()
pb <- txtProgressBar(min = 0, max = n_samples, style = 3)
for(i in 1:n_samples) {
setTxtProgressBar(pb, i)
seed2 = sample(100:1000, 1)
all_sample_df = sample_text_data(sample_size_list, seed=seed2)
all_tokens = get_gram_counts(all_sample_df$value)
final_output = final_output %>%
bind_rows(all_tokens) %>%
group_by(n_gram, token) %>%
summarize(n = sum(n))
}
return(final_output %>% arrange(desc(n)))
}
start = Sys.time()
all_tokens = build_token_count_df()
clean_vector_source = function(vector_source) {
clean_output = vector_source %>%
str_num2words %>%
str_replace_all('[^\\sA-Za-z\'\\-]', '') %>%
str_replace_all('(?<!\\b[A-Za-z]{0,9})([-])(?![A-Za-z]+\\b)', "")
clean_output = clean_output[!str_detect(clean_output, "[^\x20-\x7E]")]
return(clean_output)
}
get_all_grams = function(tbl_source) {
unigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 1, to_lower=FALSE)
unigrams$n_gram = 'one'
bigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 2, to_lower=FALSE)
bigrams$n_gram = 'two'
trigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 3, to_lower=FALSE)
trigrams$n_gram = 'three'
return(bind_rows(unigrams,bigrams,trigrams))
}
get_gram_counts = function(vector_source) {
all_tokens = data.frame(clean=clean_vector_source(vector_source), stringsAsFactors = FALSE) %>%
tbl_df %>%
get_all_grams %>%
group_by(n_gram) %>%
count(token, sort=TRUE)
return(all_tokens)
}
build_token_count_df = function(seed=1234, n_samples = 50, sample_size = 1*10^7) {
set.seed(seed)
sample_size_list = lapply(list(news, blogs, tweets), function(x) sample_size / get_med_bytes_per_row(x))
final_output = data.frame()
pb <- txtProgressBar(min = 0, max = n_samples, style = 3)
for(i in 1:n_samples) {
setTxtProgressBar(pb, i)
seed2 = sample(100:1000, 1)
all_sample_df = sample_text_data(sample_size_list, seed=seed2)
all_tokens = get_gram_counts(all_sample_df$value)
final_output = final_output %>%
bind_rows(all_tokens) %>%
group_by(n_gram, token) %>%
summarize(n = sum(n))
}
return(final_output %>% arrange(desc(n)))
}
start = Sys.time()
all_tokens = build_token_count_df()
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
library(stringr)
library(qdap)
library(tm)
library(tidytext)
news <- read_lines("Resources/corpus/en_US.news.txt") %>% tbl_df
news <- read_lines("Resources/corpus/en_US/en_US.news.txt") %>% tbl_df
blogs <- read_lines("Resources/corpus/en_US/en_US.blogs.txt") %>% tbl_df
tweets <- read_lines("Resources/corpus/en_US/en_US.twitter.txt") %>% tbl_df
# Source: https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
## Function by John Fox found here:
## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
## Tweaks by AJH to add commas and "and"
## Given a number, returns a string of the number's word equivalent
helper <- function(x){
digits <- rev(strsplit(as.character(x), "")[[1]])
nDigits <- length(digits)
if (nDigits == 1) as.vector(ones[digits])
else if (nDigits == 2)
if (x <= 19) as.vector(teens[digits[1]])
else trim(paste(tens[digits[2]],
Recall(as.numeric(digits[1]))))
else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and",
Recall(makeNumber(digits[2:1]))))
else {
nSuffix <- ((nDigits + 2) %/% 3) - 1
if (nSuffix > length(suffixes)) as.vector("")
else trim(paste(Recall(makeNumber(digits[
nDigits:(3*nSuffix + 1)])),
suffixes[nSuffix],"," ,
Recall(makeNumber(digits[(3*nSuffix):1]))))
}
}
trim <- function(text){
#Tidy leading/trailing whitespace, space before comma
text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
#Clear any trailing " and"
text=gsub(" and$","",text)
#Clear any trailing comma
gsub("\ *,$","",text)
}
makeNumber <- function(...) as.numeric(paste(..., collapse=""))
#Disable scientific notation
opts <- options(scipen=100)
on.exit(options(opts))
ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
"eight", "nine")
names(ones) <- 0:9
teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
"sixteen", " seventeen", "eighteen", "nineteen")
names(teens) <- 0:9
tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
"ninety")
names(tens) <- 2:9
x <- round(x)
suffixes <- c("thousand", "million", "billion", "trillion")
if (length(x) > 1) return(trim(sapply(x, helper)))
helper(x)
}
str_num2words = function(character_vector) {
new_vec = c()
for (input_string in character_vector) {
## Given an input string, replaces all digits with their numeric equivalents using numbers2words()
numbers = str_extract_all(input_string, '[0-9]+')[[1]]
conv_list = list()
for(num in numbers) {
conv_list[[num]] = numbers2words(as.numeric(num))
}
output = input_string
for(num in names(conv_list)) {
output = str_replace_all(output, num, conv_list[[num]])
}
new_vec = c(new_vec, output)
}
return(new_vec)
}
# 1o Helper Functions
clean_vector_source = function(vector_source) {
## Given a character vector source,
## 1) Removes input lines with non-ASCII characters
## 2) Breaks all the lines into individual sentences (by end punctuation)
## 3) Removes all punctuation other than hyphens and apostrophes
## 4) Keeps hyphens only if they are connecting a compound word, as in "Part-time student"
## 5) Replaces all digits with their word equivalent
##
## Returns a clean character vector
clean_output = vector_source[!str_detect(vector_source, "[^\x20-\x7E]")] %>%
str_split('[?!.|;]+ ') %>%
unlist %>%
str_replace_all('[^\\sA-Za-z\'\\-]', '') %>%
str_replace_all('(?<!\\b[A-Za-z]{0,9})([-])(?![A-Za-z]+\\b)', "")
#str_num2words
clean_output = clean_output[clean_output != ""]
return(clean_output)
}
get_all_grams = function(tbl_source) {
## Given an input tbl with a character vector called "clean"
## Extracts all unigrams, bigrams, and trigrams from the character vector
## Returns a single data frame containing all tokens, labelled by the number of grams
unigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 1, to_lower=FALSE)
unigrams$n_gram = 'one'
bigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 2, to_lower=FALSE)
bigrams$n_gram = 'two'
trigrams = tbl_source %>% unnest_tokens(token, clean, token="ngrams", n = 3, to_lower=FALSE)
trigrams$n_gram = 'three'
return(bind_rows(unigrams,bigrams,trigrams))
}
# 2o Helper Functions
get_gram_counts = function(vector_source) {
## Given a character vector source,
## Runs clean_vector_source and get_all_grams
## Then, counts the number of instances of each unique token
## Returns a tbl data frame of all the tokens from the input vector
## Sorted by number of occurances and labelled by the number of grams in each token
all_tokens = data.frame(clean=clean_vector_source(vector_source), stringsAsFactors = FALSE) %>%
tbl_df %>%
get_all_grams %>%
group_by(n_gram) %>%
count(token, sort=TRUE)
return(all_tokens)
}
news_grams = get_gram_counts(news$value)
write_csv(news_grams, "Resources/corpus/en_US/news_grams.csv")
blogs_grams = get_gram_counts(blogs$value)
write_csv(blogs_grams, "Resources/corpus/en_US/blogs_grams.csv")
tweets_grams = get_gram_counts(tweets$value)
write_csv(tweets_grams, "Resources/corpus/en_US/tweets_grams.csv")
rm(list=ls())
